{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install bs4\n",
    "%pip install scikit-optimize\n",
    "%pip install line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# fabric_dir = \"../fabric-samples\"\n",
    "# caliper_dir = \"../caliper-benchmarks\"\n",
    "\n",
    "\n",
    "# if os.path.isdir(fabric_dir):\n",
    "#     print(\"Directory exists\")\n",
    "# else:\n",
    "#     subprocess.run(\"cd ../ && git clone https://github.com/hyperledger/fabric-samples.git\", shell=True)\n",
    "\n",
    "# if os.path.dirname(caliper_dir):\n",
    "#     print(\"Directory exists\")\n",
    "# else:\n",
    "#     subprocess.run(\"cd ../ && git clone https://github.com/hyperledger-caliper/caliper-benchmarks.git\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subprocess.run(\"cd ../ && npm install --only=prod @hyperledger/caliper-cli\", shell=True)\n",
    "# subprocess.run(\"cd ../ && npx caliper bind --caliper-bind-sut fabric:2.4\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def get_config(path: str):\n",
    "    with open(path, 'r') as f:\n",
    "        return yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd ../caliper-benchmarks/ && npx caliper launch manager --caliper-workspace ./ --caliper-networkconfig networks/fabric/test-network.yaml --caliper-benchconfig benchmarks/samples/fabric/fabcar/config.yaml --caliper-flow-only-test --caliper-fabric-gateway-enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def convert_fabric_html_to_json(html_file, output_json_file):\n",
    "    try:\n",
    "        with open(html_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            soup = BeautifulSoup(file, \"html.parser\")\n",
    "        \n",
    "        summary = soup.find(\"div\", {\"id\": \"summary\"}).text.strip() if soup.find(\"div\", {\"id\": \"summary\"}) else \"No summary found\"\n",
    "        metrics = {}\n",
    "        \n",
    "        table = soup.find(\"table\")\n",
    "        if table:\n",
    "            headers = [th.text.strip() for th in table.find_all(\"th\")]\n",
    "            rows = table.find_all(\"tr\")[1:]\n",
    "            for row in rows:\n",
    "                cells = row.find_all(\"td\")\n",
    "                key = cells[0].text.strip()\n",
    "                values = {headers[i]: cells[i].text.strip() for i in range(1, len(cells))}\n",
    "                metrics[key] = values\n",
    "        \n",
    "        report_data = {\n",
    "            \"summary\": summary,\n",
    "            \"metrics\": metrics\n",
    "        }\n",
    "        \n",
    "        with open(output_json_file, \"w\", encoding=\"utf-8\") as json_file:\n",
    "            json.dump(report_data, json_file, indent=4)\n",
    "        \n",
    "        print(f\"JSON report successfully created: {output_json_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during conversion: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_tps(json_file):\n",
    "    try:\n",
    "        json_data = None\n",
    "        with open(json_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            json_data = json.load(file)\n",
    "        tps_values = []\n",
    "        for _, metrics in json_data[\"metrics\"].items():\n",
    "            tps_value = float(metrics.get(\"Throughput (TPS)\", 0))\n",
    "            tps_values.append(tps_value)\n",
    "        \n",
    "        if not tps_values:\n",
    "            print(\"No TPS values found.\")\n",
    "            return None\n",
    "        \n",
    "        average_tps = sum(tps_values) / len(tps_values)\n",
    "        return average_tps\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_fabric_html_to_json('../caliper-benchmarks/report.html', 'report.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_tps = calculate_average_tps('report.json')\n",
    "# avg_tps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# assumes binding is done to fabric 2.4\n",
    "def objective():\n",
    "    # start network\n",
    "    network_starter = 'cd ../fabric-samples/test-network && ./network.sh up createChannel -s couchdb'\n",
    "    subprocess.run(network_starter, shell=True)\n",
    "\n",
    "    #deploy chaincode\n",
    "    chaincode_deployer = 'cd ../fabric-samples/test-network && ./network.sh deployCC -ccn fabcar -ccp ../../caliper-benchmarks/src/fabric/samples/fabcar/go -ccl go'\n",
    "    subprocess.run(chaincode_deployer, shell=True)\n",
    "\n",
    "    # execute benchmark\n",
    "    command = \"cd ../caliper-benchmarks/ && npx caliper launch manager --caliper-workspace ./ --caliper-networkconfig networks/fabric/test-network.yaml --caliper-benchconfig benchmarks/samples/fabric/fabcar/config.yaml --caliper-flow-only-test --caliper-fabric-gateway-enabled\"\n",
    "    subprocess.run(command, shell=True)\n",
    "\n",
    "    # process output\n",
    "    convert_fabric_html_to_json('../caliper-benchmarks/report.html', 'report.json')\n",
    "    avg_tps = calculate_average_tps('report.json')\n",
    "\n",
    "    # shut down network\n",
    "    shutter = 'cd ../fabric-samples/test-network && ./network.sh down'\n",
    "    subprocess.run(shutter, shell=True)\n",
    "    return avg_tps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = objective()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skopt import Optimizer\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "def fill_config(conf, next_point, sample_params):\n",
    "    for i in range(len(sample_params)):\n",
    "        internal_path = sample_params[i].name.split('.')\n",
    "        parts = internal_path[-1].split('|')\n",
    "        assert(len(parts) > 0 and len(parts) <= 2)\n",
    "        internal_path[-1] = parts[0]\n",
    "\n",
    "        assert(len(internal_path) > 0)\n",
    "        link = conf[internal_path[0]]\n",
    "        for j in range(1, len(internal_path)):\n",
    "            link = link[internal_path[j]]\n",
    "        # print(link)\n",
    "        if len(parts) == 1:\n",
    "            link = next_point[i]\n",
    "        else:\n",
    "            link = str(int(next_point[i])) + parts[1]\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = '/home/admin1/hse/blockchain-benchmarks/fabric-samples/test-network/configtx/configtx.yaml'\n",
    "conf = get_config(str(CONFIG_PATH))\n",
    "# assert(False, \"Config path\")\n",
    "\n",
    "# TODO: automate config\n",
    "\n",
    "sample_params = [\n",
    "    Integer(1, 19, name=\"Orderer.BatchSize.MaxMessageCount|\"), # follow style of conf\n",
    "    Integer(1, 49, name=\"Orderer.BatchSize.AbsoluteMaxBytes|MB\"), # 49 is max recommended\n",
    "    Integer(1, 2048, name=\"Orderer.BatchSize.PreferredMaxBytes|KB\"),\n",
    "    Integer(1, 30, name=\"Orderer.BatchTimeout|s\"),\n",
    "    Integer(1, 999, name=\"Profiles.ChannelUsingRaft.Orderer.EtcdRaft.Options.TickInterval|ms\"),\n",
    "    Integer(1, 100, name=\"Profiles.ChannelUsingRaft.Orderer.EtcdRaft.Options.ElectionTick\"),\n",
    "    Integer(1, 10, name=\"Profiles.ChannelUsingRaft.Orderer.EtcdRaft.Options.HeartbeatTick\"),\n",
    "    Integer(1, 50, name=\"Profiles.ChannelUsingRaft.Orderer.EtcdRaft.Options.MaxInflightBlocks\"),\n",
    "    Integer(1, 128, name=\"Profiles.ChannelUsingRaft.Orderer.EtcdRaft.Options.SnapshotIntervalSize|MB\"),\n",
    "]\n",
    "\n",
    "# TODO: значимость параметров -- SHAP-score\n",
    "\n",
    "# TODO: orderer.type == BFT\n",
    "\n",
    "# обучение с привилегированными данными\n",
    "# data fusion\n",
    "# априорное распределение модели\n",
    "\n",
    "optimizer = Optimizer(\n",
    "    dimensions=sample_params,\n",
    "    base_estimator=\"GP\",\n",
    "    n_initial_points=10,\n",
    "    acq_func=\"gp_hedge\",\n",
    "    acq_optimizer=\"auto\",\n",
    ")\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "# learning\n",
    "\n",
    "def fit(sample_params, optimizer, conf, num_epochs):\n",
    "\n",
    "    scores = []\n",
    "    optimals = [] # TODO: file\n",
    "\n",
    "    for i in range(NUM_EPOCHS): # tqdm\n",
    "        next_point = optimizer.ask()\n",
    "\n",
    "        if next_point is None:\n",
    "            print(\"No more points to evaluate.\")\n",
    "            break\n",
    "\n",
    "        # fill config with better(?) params\n",
    "        fill_config(conf, next_point, sample_params)\n",
    "\n",
    "        # save config\n",
    "        #TODO: why do it\n",
    "        # with open(CONFIG_PATH, 'w') as f:\n",
    "        #     yaml.dump(conf, f)\n",
    "\n",
    "        # mop\n",
    "        print(f\"Next point to evaluate: {next_point}\")\n",
    "        optimals.append(next_point)\n",
    "        value = objective() # 10 objectives for snr\n",
    "        scores.append(value)\n",
    "        print(f\"Score: {value}\\n\")\n",
    "        optimizer.tell(next_point, -value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from line_profiler import LineProfiler\n",
    "\n",
    "profiler = LineProfiler()\n",
    "profiler.add_function(fit)\n",
    "profiler.enable()\n",
    "fit(sample_params, optimizer, conf, NUM_EPOCHS)\n",
    "profiler.disable()\n",
    "profiler.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.plot(scores, marker='o', linestyle='-', color='b', label='scores')\n",
    "\n",
    "# plt.title(\"Graph of ml\", fontsize=14)\n",
    "\n",
    "# plt.xlabel(\"Index\", fontsize=12)\n",
    "# plt.ylabel(\"Value\", fontsize=12)\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.plot(optimals, marker='o', linestyle='-', color='b', label='scores')\n",
    "\n",
    "# plt.title(\"Graph of ml\", fontsize=14)\n",
    "# plt.xlabel(\"Index\", fontsize=12)\n",
    "# plt.ylabel(\"Value\", fontsize=12)\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def snr(scores_global):\n",
    "#     for scores in scores_global:\n",
    "#         mean_score = np.mean(scores)\n",
    "#         std_dev = np.std(scores, ddof=1)\n",
    "\n",
    "#         if std_dev == 0:\n",
    "#             return float('inf')  # Return infinity if no variation\n",
    "\n",
    "#         print(mean_score / std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.7622677397895234"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "arr = []\n",
    "with open(\"scores_snr.txt\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line[1:-2]\n",
    "        for x in line.split(\", \"):\n",
    "            arr.append(float(x))\n",
    "\n",
    "noise_value = np.std(arr, ddof=1)\n",
    "mean_score = np.mean(arr)\n",
    "relative_noise = (noise_value / mean_score) * 100\n",
    "\n",
    "relative_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr = arr[0:-2]\n",
    "# noise_value = np.std(arr, ddof=1)\n",
    "# mean_score = np.mean(arr)\n",
    "# relative_noise = (noise_value / mean_score) * 100 \n",
    "\n",
    "# relative_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.272535542675445"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal = np.mean([330.525, 321.925])\n",
    "snr = signal / noise_value\n",
    "snr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
